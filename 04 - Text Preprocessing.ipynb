{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import pprint\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import glob\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from pandas.io.json import json_normalize\n",
    "import cPickle as pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "%matplotlib inline\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#nlp\n",
    "import enchant\n",
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./data/processed/unique_all_tweets_filtered.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Replacement Dictionaries\n",
    "First load and generate dictionaries for replacing twitter and internet specific slang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the twitter slang replacements which have been generated by analyzing the text\n",
    "with open('./data/twitterSlang.csv','r') as f:\n",
    "    reader = csv.reader(f,delimiter=',')\n",
    "    slangTwitter = {row[0]:row[1] for row in reader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Build a slang dictionary based on the website\n",
    "#In total 1461 slang words are described on this side.\n",
    "#The following lines download the content, parse the HTML table and put all information into a python dictionary \n",
    "response = requests.get('http://www.webopedia.com/quick_ref/textmessageabbreviations.asp').content\n",
    "soup = BeautifulSoup(response,'html.parser')\n",
    "slangDict = {row.select('td')[0].getText().strip().lower():row.select('td')[1].getText().strip().lower() for row in soup.select('table tr') if len(row.select('td'))>1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "apostropheDictionary = {\n",
    "    \"'s\":' is',\n",
    "    \"'t\":' not',\n",
    "    \"'re\":' are',\n",
    "    \"'d\":\" had\",\n",
    "    \"'ll\":\" will\",\n",
    "    \"'m\":\" am\",\n",
    "    \"'ve\":\" have\",\n",
    "    \"im\":\" i am\",\n",
    "    \"i've\":\" i have\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#taken from wikipedia https://en.wikipedia.org/wiki/Wikipedia%3aList_of_English_contractions\n",
    "with open('./data/english_contractions.json','r') as f:\n",
    "    contractions = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Different Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeLinebreaks(tweet):\n",
    "    return tweet.replace('\\n', ' ').replace('\\r', '').rstrip().strip()\n",
    "\n",
    "rpt_regex = re.compile(r\"(.)\\1{1,}\", re.IGNORECASE)\n",
    "def removeDuplicatedCharacters(tweet):\n",
    "    \"\"\"\n",
    "    E.g. \"i am happppy\" => \"i am happy\"\n",
    "    Even if not all characters are removed, like \"hallo\" => \"halo\", the spelling correction is applied to get a true word\n",
    "    \"\"\"\n",
    "    return rpt_regex.sub(r\"\\1\\1\", tweet)\n",
    "\n",
    "def extractHashtags(tweet):\n",
    "    return ' '.join([x['text'] for x in tweet.get('entities',{}).get('hashtags')]).lower()\n",
    "\n",
    "def checkEmoticons(attitude,tweet_text):\n",
    "    found = False\n",
    "    for emoticon in config.get('datasets',{}).get('emoticons',{}).get(attitude,{}).get('replace',[]):\n",
    "        if emoticon in tweet_text:\n",
    "            found = True \n",
    "    if not found:\n",
    "        print tweet_text\n",
    "        \n",
    "def removeEmoticons(tweet):\n",
    "    \"\"\"\n",
    "    Removes the emoticons from the tweets.\n",
    "    E.g. \"I really like it :-)\" => \"I really like it\"\n",
    "    Does not handle emoticons like \":-))))))\" but those will be removed in a final step after all preprocessing\n",
    "    \"\"\"\n",
    "    return re.sub(r'(\\:\\w+\\:|\\<[\\/\\\\]?3|[\\(\\)\\\\\\D|\\*\\$][\\-\\^]?[\\:\\;\\=]|[\\:\\;\\=B8][\\-\\^]?[3DOPp\\@\\$\\*\\\\\\)\\(\\/\\|])(?=\\s|[\\!\\.\\?]|$)','',tweet)\n",
    "\n",
    "def apostropheLookup(tweet):\n",
    "    \"\"\"\n",
    "    Maintains a common usage of apostophes\n",
    "    E.g. \"I've been happy\" => \"I have been happy\"\n",
    "    First try to find the exact contraction in the wikipedia list, and then try to replace by smaller apostrophe dict\n",
    "    \"\"\"\n",
    "    tweet = ' '.join(contractions[word] if word in contractions else word for word in tweet.split())\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    return ' '.join([apostropheDictionary[token] if token in apostropheDictionary else token for token in tokens])\n",
    "\n",
    "def replaceSlang(tweet):\n",
    "    \"\"\"\n",
    "    Replaces the slang in the tweet based on the previously generated dictionary from a online website describing\n",
    "    the different slang words.\n",
    "    Replacement is done by the written out slang\n",
    "    But only replace if the token length is min 2 characters long => do not replace numbers, questionmarks, etc.\n",
    "    \"\"\"\n",
    "    tokens = tweet.split(' ')\n",
    "    return ' '.join([slangDict[token.lower()] if token.lower() in slangDict and len(token)>2 else token for token in tokens])\n",
    "\n",
    "def replaceTwitterSlang(tweet):\n",
    "    \"\"\"\n",
    "    Replaces twitter specific slang, based on a dictionary created from various online sources\n",
    "    \"\"\"\n",
    "    tokens = tweet.split(' ')\n",
    "    return ' '.join([slangTwitter[token.lower()] if token.lower() in slangTwitter else token for token in tokens])\n",
    "\n",
    "#matches 1114 emojis, taken from http://stackoverflow.com/questions/43242440/javascript-unicode-emoji-regular-expressions\n",
    "emojiRegex = r'(?:[\\u00A9\\u00AE\\u203C\\u2049\\u2122\\u2139\\u2194-\\u2199\\u21A9-\\u21AA\\u231A-\\u231B\\u2328\\u23CF\\u23E9-\\u23F3\\u23F8-\\u23FA\\u24C2\\u25AA-\\u25AB\\u25B6\\u25C0\\u25FB-\\u25FE\\u2600-\\u2604\\u260E\\u2611\\u2614-\\u2615\\u2618\\u261D\\u2620\\u2622-\\u2623\\u2626\\u262A\\u262E-\\u262F\\u2638-\\u263A\\u2640\\u2642\\u2648-\\u2653\\u2660\\u2663\\u2665-\\u2666\\u2668\\u267B\\u267F\\u2692-\\u2697\\u2699\\u269B-\\u269C\\u26A0-\\u26A1\\u26AA-\\u26AB\\u26B0-\\u26B1\\u26BD-\\u26BE\\u26C4-\\u26C5\\u26C8\\u26CE-\\u26CF\\u26D1\\u26D3-\\u26D4\\u26E9-\\u26EA\\u26F0-\\u26F5\\u26F7-\\u26FA\\u26FD\\u2702\\u2705\\u2708-\\u270D\\u270F\\u2712\\u2714\\u2716\\u271D\\u2721\\u2728\\u2733-\\u2734\\u2744\\u2747\\u274C\\u274E\\u2753-\\u2755\\u2757\\u2763-\\u2764\\u2795-\\u2797\\u27A1\\u27B0\\u27BF\\u2934-\\u2935\\u2B05-\\u2B07\\u2B1B-\\u2B1C\\u2B50\\u2B55\\u3030\\u303D\\u3297\\u3299]|(?:\\uD83C[\\uDC04\\uDCCF\\uDD70-\\uDD71\\uDD7E-\\uDD7F\\uDD8E\\uDD91-\\uDD9A\\uDDE6-\\uDDFF\\uDE01-\\uDE02\\uDE1A\\uDE2F\\uDE32-\\uDE3A\\uDE50-\\uDE51\\uDF00-\\uDF21\\uDF24-\\uDF93\\uDF96-\\uDF97\\uDF99-\\uDF9B\\uDF9E-\\uDFF0\\uDFF3-\\uDFF5\\uDFF7-\\uDFFF]|\\uD83D[\\uDC00-\\uDCFD\\uDCFF-\\uDD3D\\uDD49-\\uDD4E\\uDD50-\\uDD67\\uDD6F-\\uDD70\\uDD73-\\uDD7A\\uDD87\\uDD8A-\\uDD8D\\uDD90\\uDD95-\\uDD96\\uDDA4-\\uDDA5\\uDDA8\\uDDB1-\\uDDB2\\uDDBC\\uDDC2-\\uDDC4\\uDDD1-\\uDDD3\\uDDDC-\\uDDDE\\uDDE1\\uDDE3\\uDDE8\\uDDEF\\uDDF3\\uDDFA-\\uDE4F\\uDE80-\\uDEC5\\uDECB-\\uDED2\\uDEE0-\\uDEE5\\uDEE9\\uDEEB-\\uDEEC\\uDEF0\\uDEF3-\\uDEF6]|\\uD83E[\\uDD10-\\uDD1E\\uDD20-\\uDD27\\uDD30\\uDD33-\\uDD3A\\uDD3C-\\uDD3E\\uDD40-\\uDD45\\uDD47-\\uDD4B\\uDD50-\\uDD5E\\uDD80-\\uDD91\\uDDC0]))'\n",
    "emoji_pattern = re.compile(\n",
    "    u\"(\\ud83d[\\ude00-\\ude4f])|\"  # emoticons\n",
    "    u\"(\\ud83c[\\udf00-\\uffff])|\"  # symbols & pictographs (1 of 2)\n",
    "    u\"(\\ud83d[\\u0000-\\uddff])|\"  # symbols & pictographs (2 of 2)\n",
    "    u\"(\\ud83d[\\ude80-\\udeff])|\"  # transport & map symbols\n",
    "    u\"(\\ud83c[\\udde0-\\uddff])|\"  # flags (iOS)\n",
    "    u\"(\\udf00-\\uffff)\"           #wrong\n",
    "    \"+\")\n",
    "def extractEmojis(tweet):\n",
    "    \"\"\"\n",
    "    Extracts emojies from tweet\n",
    "    \"\"\"\n",
    "    emojies = emoji_pattern.findall(tweet)\n",
    "    #put all in array\n",
    "    return [emoji for sublist in [list(emoji) for emoji in emojies] for emoji in sublist if emoji!='']\n",
    "\n",
    "def splitEmojis(tweet):\n",
    "    \"\"\"\n",
    "    Sometimes emojies are concanated and the tokenizer can not split them\n",
    "    This function inserts spaces before and after each emoji\n",
    "    \"\"\"\n",
    "    emojies = extractEmojis(tweet)\n",
    "    for emoji in emojies:\n",
    "        tweet = tweet.replace(emoji,' '+emoji+' ')\n",
    "        #remove wrong emoji\n",
    "        if emoji==u'\\ud83c' or emoji==u'\\uddfa':\n",
    "            tweet = tweet.replace(emoji,'')\n",
    "    return tweet\n",
    "\n",
    "def spellingCorrection(tweet,whitelist):\n",
    "    \"\"\"\n",
    "    Correction of tweet spelling.\n",
    "    E.g. \"speling\" => \"spelling\"\n",
    "    Only if the word is longer than 3 characters=> do not try to correct abbreviations or other small words\n",
    "    And only if the word does not appear within the top 50% of words => do not change iphone to phone, etc.\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    try:\n",
    "        correction = ' '.join([word if dictionary.check(word) else dictionary.suggest(word)[0] if len(word)>3 and word not in whitelist and not word.startswith(\"xx\") and not (word.startswith('\\u') or word.startswith('\\U')) and len(dictionary.suggest(word))>0 else word for word in tokens])\n",
    "        return correction\n",
    "    except:\n",
    "        return tweet\n",
    "\n",
    "translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "def removePunctuationAndMisc(tweet):\n",
    "    \"\"\"\n",
    "    Removes all the punctuation from the string and clean duplicate spaces, strip, etc.\n",
    "    E.g. \"... i dont like it\" => \"i dont like it\"\n",
    "    \"\"\"\n",
    "    return re.sub(\"\\s\\s+\" , \" \", ''.join([i for i in tweet.translate(translate_table) if not i.isdigit()])).strip()\n",
    "\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "lmtzr = WordNetLemmatizer()\n",
    "def lemmatize_tweet(tweet):\n",
    "    \"\"\"\n",
    "    Lemmatizes the given tweet using nltk WordNetLemmatizer\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(tweet)\n",
    "    pos_tagged_words = pos_tag(tokens)\n",
    "    lemmatized_tweet = [lmtzr.lemmatize(i[0],get_wordnet_pos(i[1])) for i in pos_tagged_words]\n",
    "    return ' '.join(lemmatized_tweet).strip()\n",
    "\n",
    "def replaceEntities(tweet,topic):\n",
    "    \"\"\"\n",
    "    Replaces twitter specific entities like hashtags, mentions, \n",
    "    urls, etc. and replaces them by static string\n",
    "    \"\"\"\n",
    "    #pprint.pprint(tweet)\n",
    "    temp_indices = {}\n",
    "    processed_tweet = tweet.get('full_text')\n",
    "    \n",
    "    for entity_name,entity_object in tweet.get('entities',{}).iteritems():\n",
    "        if entity_object:\n",
    "            replace_string = \" XX\"+entity_name.upper()+\"XX \"\n",
    "            for entity in entity_object:\n",
    "                indices = entity.get('indices')\n",
    "                temp_indices[indices[0]] = (replace_string,indices[1])\n",
    "    #replace the entities backwards in order to maintain the indices\n",
    "    for position in sorted(temp_indices.iterkeys(),reverse=True):\n",
    "        processed_tweet = processed_tweet[:position]+temp_indices[position][0]+processed_tweet[temp_indices[position][1]:]\n",
    "\n",
    "\n",
    "    #replace topic, only whole words\n",
    "    processed_tweet = re.sub(r'\\b%s\\b' % topic, ' XXTOPICXX ',processed_tweet,flags=re.I)\n",
    "    return processed_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123457\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a51d057fcb14d94b65b10974abc08b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056da2fe684b4bdab2c641c81d0b514c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "from itertools import islice\n",
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))\n",
    "\n",
    "\n",
    "#datasubet = take(10, data.iteritems())\n",
    "\n",
    "\n",
    "from ipywidgets import FloatProgress\n",
    "from IPython.display import display\n",
    "progress = FloatProgress(min=0, max=len(data),description=\"Processing the text\")\n",
    "display(progress)\n",
    "\n",
    "\n",
    "\n",
    "dictionary = enchant.Dict(\"en_US\")\n",
    "#helper dictionary to count the corpus size after each processing step\n",
    "corpus_tokens = {\n",
    "    \"00_raw\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"01_replaceEntities\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"02_misc\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"03_removeEmoticons\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"04_removeDuplicatedCharacters\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"05_apostropheLookup\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"06_replaceSlang\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"07_replaceTwitterSlang\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"08_splitEmojis\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"09_removePunctuationAndMisc\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"10_spellingCorrection\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    },\n",
    "    \"11_lemmatization\":{\n",
    "        \"tokens\":set(),\n",
    "        \"touched\":0\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def didChange(oldtweet,newtweet):\n",
    "    if oldtweet==newtweet:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "for tweet_id,tweet in data.iteritems():\n",
    "    progress.value += 1\n",
    "    \n",
    "    \n",
    "    corpus_tokens['00_raw']['tokens'].update(nltk.word_tokenize(tweet['full_text']))\n",
    "    \n",
    "    tweet[\"extracted_hashtags\"] = extractHashtags(tweet)\n",
    "    \n",
    "    #replace entities with placeholders\n",
    "    replace_entities_text = tweet[\"processed_tweet\"] = replaceEntities(tweet,tweet['category']['seed_topic'])\n",
    "    corpus_tokens['01_replaceEntities']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['01_replaceEntities']['touched'] += didChange(tweet['full_text'],tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #Misc preprocessing\n",
    "    tweet[\"processed_tweet\"] = tweet[\"processed_tweet\"].lower()\n",
    "    misc_text = tweet[\"processed_tweet\"] = removeLinebreaks(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['02_misc']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['02_misc']['touched'] += didChange(replace_entities_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #remove emoticons from tweets\n",
    "    removeEmoticonText = tweet[\"processed_tweet\"] = removeEmoticons(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['03_removeEmoticons']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['03_removeEmoticons']['touched'] += didChange(misc_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #remove duplicated characters\n",
    "    removeDuplicateText = tweet[\"processed_tweet\"] = removeDuplicatedCharacters(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['04_removeDuplicatedCharacters']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['04_removeDuplicatedCharacters']['touched'] += didChange(removeEmoticonText,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #remove apostrophes and replace by written word\n",
    "    apostrophe_text = tweet[\"processed_tweet\"] = apostropheLookup(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['05_apostropheLookup']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['05_apostropheLookup']['touched'] += didChange(removeDuplicateText,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #replace slang\n",
    "    replaceslang_text = tweet[\"processed_tweet\"] = replaceSlang(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['06_replaceSlang']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['06_replaceSlang']['touched'] += didChange(apostrophe_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #replace twitter speicifc slang\n",
    "    replacetwitterslang_text = tweet[\"processed_tweet\"] = replaceTwitterSlang(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['07_replaceTwitterSlang']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['07_replaceTwitterSlang']['touched'] += didChange(replaceslang_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #split emojis\n",
    "    splitemoji_text = tweet[\"processed_tweet\"] = splitEmojis(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['08_splitEmojis']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['08_splitEmojis']['touched'] += didChange(replacetwitterslang_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "    #remove punctuation\n",
    "    tweet[\"processed_tweet\"] = removePunctuationAndMisc(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['09_removePunctuationAndMisc']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['09_removePunctuationAndMisc']['touched'] += didChange(splitemoji_text,tweet[\"processed_tweet\"])\n",
    "    \n",
    "spelling_progress = FloatProgress(min=0, max=len(data),description=\"Spellingcorrection and Lemmatization\")\n",
    "display(spelling_progress)\n",
    "\n",
    "#Create a corpus of all words in the tweets\n",
    "#Just as a helper for other functions\n",
    "tweet_dictionary = [token for sublist in [nltk.word_tokenize(tweet[\"processed_tweet\"]) for tweet_id,tweet in data.iteritems()] for token in sublist]\n",
    "tweet_dictionary_counter = Counter(tweet_dictionary)\n",
    "most_common_words = [word for word,count in tweet_dictionary_counter.items() if count<4]\n",
    "#perform a second loop over the processed_tweets because the dictionary has to be created on the complete data\n",
    "\n",
    "for tweet_id,tweet in data.iteritems():\n",
    "    spelling_progress.value += 1\n",
    "    #spelling at the end, after the tweets have been processed\n",
    "    old_text = tweet[\"processed_tweet\"]\n",
    "    spellcorrection_text = tweet[\"processed_tweet\"] = spellingCorrection(tweet[\"processed_tweet\"],whitelist=most_common_words)\n",
    "    corpus_tokens['10_spellingCorrection']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['10_spellingCorrection']['touched'] += didChange(old_text,tweet[\"processed_tweet\"])\n",
    "\n",
    "    #lemmatization\n",
    "    tweet[\"processed_tweet\"] = lemmatize_tweet(tweet[\"processed_tweet\"])\n",
    "    corpus_tokens['11_lemmatization']['tokens'].update(nltk.word_tokenize(tweet[\"processed_tweet\"]))\n",
    "    corpus_tokens['11_lemmatization']['touched'] += didChange(spellcorrection_text,tweet[\"processed_tweet\"])\n",
    "\n",
    "counter = {}\n",
    "for key,value in corpus_tokens.iteritems():\n",
    "    counter[key] = {\n",
    "        \"counter\":len(value['tokens']),\n",
    "        \"touched\":value['touched']\n",
    "    }\n",
    "pprint(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Final Preprocessed Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "export = [{\n",
    "    \"id\":tweet_id,\n",
    "    \"hashtags\":tweet['extracted_hashtags'],\n",
    "    \"raw_text\":removeLinebreaks(tweet['full_text']),\n",
    "    \"processed_text\":tweet['processed_tweet'],\n",
    "    \"topic\":tweet['category']['top_topic'],\n",
    "    \"seed_topic\":tweet['category']['seed_topic'],\n",
    "    \"attitude\":tweet['category']['attitude']\n",
    "} for tweet_id,tweet in data.iteritems()]\n",
    "df_export = pd.DataFrame.from_dict(export)\n",
    "df_export.to_csv(\"./data/processed/final_dataset.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Subset of Data for Manual Labelling\n",
    "One percent of the complete data set will be sampled.\n",
    "The distribution of topic and attitude is taken into account in oder to maintain the same characteristics for the subset dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tempdf = df_export.groupby(['topic','attitude']).size().reset_index(name=\"total_count\")\n",
    "tempdf = tempdf.merge(df_export.groupby('topic').size().reset_index(name=\"total_topic\"),how=\"left\",on=\"topic\")\n",
    "tempdf['amount_to_select'] = (tempdf['total_count']*0.01).astype('int')\n",
    "\n",
    "df_subset = pd.DataFrame()\n",
    "#make the subset selection\n",
    "for index, row in tempdf.iterrows():\n",
    "    df_subset = pd.concat([df_subset,df_export[(df_export['topic']==row['topic']) &(df_export['attitude']==row['attitude']) ].sample(n=row['amount_to_select'],random_state=4)])\n",
    "\n",
    "df_subset.to_csv(\"./data/processed/final_dataset_subset.csv\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
